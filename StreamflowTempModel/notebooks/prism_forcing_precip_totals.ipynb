{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcing Extraction Tutorial\n",
    "\n",
    "In this notebook, we'll present an example of converting PRISM precipitation, mean temperature, and mean dew point data into a format that can be used easily in the types of vadose and groundwater zone models we cover in the [custom zone models tutorial](custom_zone_models.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import gdal\n",
    "import os\n",
    "import numpy as np\n",
    "import fiona\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "from os.path import dirname\n",
    "import glob\n",
    "import sys\n",
    "import pickle\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "parent_dir = dirname(dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','lib'))\n",
    "rew_config = pickle.load( open( os.path.join(parent_dir,'model_data','rew_config.p'), \"rb\" ) )\n",
    "import zonal_stats as zs\n",
    "import meteolib as meteo\n",
    "import evaplib as evap\n",
    "\n",
    "model_config = pickle.load( open( os.path.join(parent_dir, 'model_data', 'model_config.p'), 'rb'))\n",
    "start_date = model_config['start_date']\n",
    "stop_date = model_config['stop_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to specify the geographic areas of interest, comprised of the collection of REW sub-basins. We'll assume that these regions are specified via shapefiles located in the `raw_data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "except RuntimeError:\n",
    "    print 'Cannot find basins shapefile. Please make sure basins shapefile is located in \\n the model directory under /raw_data/basins_poly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we'll assume that the forcing along the spatial extent of each basin is constant and equal to the forcing observed at its centroid. We could also consider implementing zonal averaging or some other more faithful approximation, but this would be much more computationally intensive, and quite likely unnecessary for lower resolution climate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO CONSIDER CHANGING THIS TO JUST READ FROM THE CENTROIDS ALREADY GENERATED\n",
    "# ALSO NOTE: new projection does not return lat/long points for xy coordinates\n",
    "fc = fiona.open(basins)\n",
    "shapefile_record = fc.next()\n",
    "pos_dict={}\n",
    "for shapefile_record in fc:\n",
    "    shape = shapely.geometry.asShape(shapefile_record['geometry'])\n",
    "    long_point = shape.centroid.coords.xy[0][0]\n",
    "    lat_point = shape.centroid.coords.xy[1][0]\n",
    "    pos = (long_point, lat_point)\n",
    "    pos_dict[int(shapefile_record['properties']['cat'])]=pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the list of centroids in hand, we can begin unpacking the forcing data. Here we'll focus on three time series: daily precipitation (`ppt`), mean daily temperature (`tmean`), and mean daily dew point (`tdmean`). \n",
    "\n",
    "Our main data structure here will be a dictionary whose keys are forcing data types and whose values are themselves dictionaries. These inner dictionaries have REW IDs as keys, and forcing data at particular instances of time as values.\n",
    "\n",
    "We track time in a separate dictionary, again keyed based on forcing data. The values here are lists of dates at which each type of forcing data was observed. \n",
    "\n",
    "Note: we assume that each forcing time series is stored exactly as would be downloaded from the PRISM download page. A simple bash script for downloading daily PRISM data can be found [HERE](https://github.com/daviddralle/downloadPrism). The files must be unzipped after download. For the purposes of the model, the data should be stored in the `raw_data` folder in a foldername equal to the variable name, for instance precipitation (`ppt`) is in `raw_data/ppt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pos_dict = {1:(444800, 4397800), 2:(459400, 4380000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 8, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ppt data...\n",
      "2012 ppt data processed.\n",
      "2013 ppt data processed.\n",
      "2014 ppt data processed.\n",
      "2015 ppt data processed.\n",
      "2016 ppt data processed.\n",
      "2017 ppt data processed.\n",
      "Extracting tmean data...\n",
      "2012 tmean data processed.\n",
      "2013 tmean data processed.\n",
      "2014 tmean data processed.\n",
      "2015 tmean data processed.\n",
      "2016 tmean data processed.\n",
      "2017 tmean data processed.\n",
      "Extracting tmin data...\n",
      "2012 tmin data processed.\n",
      "2013 tmin data processed.\n",
      "2014 tmin data processed.\n",
      "2015 tmin data processed.\n",
      "2016 tmin data processed.\n",
      "2017 tmin data processed.\n",
      "Extracting tmax data...\n",
      "2012 tmax data processed.\n",
      "2013 tmax data processed.\n",
      "2014 tmax data processed.\n",
      "2015 tmax data processed.\n",
      "2016 tmax data processed.\n",
      "2017 tmax data processed.\n"
     ]
    }
   ],
   "source": [
    "forcing_dict = {}\n",
    "dates_dict = {}\n",
    "\n",
    "start_date = model_config['start_date']\n",
    "stop_date = datetime.date(pd.to_datetime('2017-09-30'))\n",
    "\n",
    "prism_vars = ['ppt','tmean', 'tmin', 'tmax']\n",
    "\n",
    "for prism_var in prism_vars:\n",
    "    print \"Extracting \" + str(prism_var) + \" data...\"\n",
    "    years = os.listdir(os.path.join(parent_dir,'raw_data',prism_var))\n",
    "\n",
    "    # Initialize empty data structures -- a dict keyed on REW ID with empty lists as values, \n",
    "    # and an empty list that will hold dates at which prism_var was observed\n",
    "    vals_dict = {k:[] for k in pos_dict.keys()}\n",
    "    date_list=[]\n",
    "    \n",
    "    #for all years of forcing variable, load each day of raster data and extract to all REWs\n",
    "    for year in years:\n",
    "        try: \n",
    "            int(year)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # Assuming that data is in .tif format from PRISM.\n",
    "        # Data for each REW is extracted to REW centroid lat/long. \n",
    "        raster_list = glob.glob(os.path.join(parent_dir,'raw_data',prism_var,year,'*.tif'))\n",
    "        for rast in raster_list:\n",
    "            datecurr = pd.to_datetime(rast[-16:][:8])\n",
    "            if (datecurr.date() > stop_date)|(datecurr.date() < start_date):\n",
    "                continue\n",
    "            date_list.append(rast[-16:][:8])\n",
    "            raster_file = os.path.join(parent_dir,'raw_data',prism_var,year,rast)\n",
    "            gdata = gdal.Open(raster_file)\n",
    "            gt = gdata.GetGeoTransform()\n",
    "            data = gdata.ReadAsArray().astype(np.float)\n",
    "            gdata = None\n",
    "            for rew_id in pos_dict.keys(): \n",
    "                pos = pos_dict[rew_id]\n",
    "                x = int((pos[0] - gt[0])/gt[1])\n",
    "                y = int((pos[1] - gt[3])/gt[5])\n",
    "                vals_dict[rew_id].append(data[y, x])\n",
    "                \n",
    "        print str(year) + ' ' + prism_var + ' data processed.'                \n",
    "\n",
    "    forcing_dict[prism_var] = vals_dict\n",
    "    dates_dict[prism_var] = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339.65000057\n",
      "1468.09000425\n",
      "2866.27000164\n",
      "3181.20599085\n"
     ]
    }
   ],
   "source": [
    "ppt_sites = forcing_dict['ppt']\n",
    "elder_rainfall = pd.DataFrame(forcing_dict['ppt'][1], index=pd.to_datetime(dates_dict['ppt']), columns=['ppt'])\n",
    "dry_rainfall = pd.DataFrame(forcing_dict['ppt'][2], index=pd.to_datetime(dates_dict['ppt']), columns=['ppt'])\n",
    "print dry_rainfall.ppt.loc['10-2014':'9-2015'].sum()\n",
    "print elder_rainfall.ppt.loc['10-2014':'9-2015'].sum()\n",
    "\n",
    "print dry_rainfall.ppt.loc['10-2016':'9-2017'].sum()\n",
    "print elder_rainfall.ppt.loc['10-2016':'9-2017'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the data for modeling, we need to ensure that the observation dates for each forcing time series align. We leverage Python's sets to make sure that all the datelists are in fact the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates match!\n"
     ]
    }
   ],
   "source": [
    "unique_dates = [list(i) for i in set(tuple(i) for i in dates_dict.values())]\n",
    "if len(unique_dates)>1:\n",
    "    print 'Forcing data dates do not all match, please check raw data!'\n",
    "else:\n",
    "    print 'Dates match!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we will house all timeseries data in Pandas dataframes in order to leverage some very nice resampling functionality. For now, we'll just initialize one empty dataframe for each REW. Each of these dataframes will be indexed by date and will have one column for each forcing timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = pd.date_range(unique_dates[0][0],unique_dates[0][-1])\n",
    "rew_dfs_dict = {k:pd.DataFrame(data=None,index=rng) for k in pos_dict.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simply cycle over the PRISM variables we enumerated earlier. Here, we might have to convert units to be in line with the main modeling effort's assumption that lengths are in units of cm and time is in units of days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for prism_var in prism_vars:\n",
    "    #need to convert ppt from mm/day to cm/day for PRISM data\n",
    "    if prism_var=='ppt': div = 10.0\n",
    "    else: div = 1.0\n",
    "    for rew_id in pos_dict.keys():\n",
    "        rew_dfs_dict[rew_id][prism_var] = np.array(forcing_dict[prism_var][rew_id])/div\n",
    "        \n",
    "# Remove any intercepted rainfall \n",
    "for rew_id in rew_dfs_dict.keys():\n",
    "    maxvals = rew_dfs_dict[rew_id].apply(lambda row: np.max([row['ppt'] - rew_config[rew_id]['interception_factor'], 0]),axis=1)\n",
    "    rew_dfs_dict[rew_id].ix[:,'ppt'] = maxvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last remaining forcing timeseries required by the model presented in this tutorial is daily average potential evapotranspiration (in units of cm/day). Here, we'll use the Priestly-Taylor equation to compute this forcing.   Before we get started, we need to build some helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute relative humidity using temp and dewpoint\n",
    "def get_rh(tmean, tdmean):\n",
    "    a = 17.625\n",
    "    b = 243.04\n",
    "    return 100*np.exp(a*tdmean/(b+tdmean))/np.exp(a*tmean/(b+tmean))\n",
    "\n",
    "def get_ea(tdmean):\n",
    "    return 0.6108*np.exp((17.27*tdmean)/(tdmean+237.3))\n",
    "\n",
    "def get_rew_pressures(rew_elevations):\n",
    "    #elevations in meters, output pressure in Pa\n",
    "    #using simple elevation/pressure relationship\n",
    "    rew_pressures = {}\n",
    "    for key in rew_elevations.keys():\n",
    "        rew_pressures[key] = 1000*101.325*((293-0.0065*rew_elevations[key])/293)**5.26\n",
    "\n",
    "    return rew_pressures\n",
    "\n",
    "# translate feature IDs to REW IDs\n",
    "def translate_to_fid(parent_dir):\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "    translated = {}\n",
    "\n",
    "    fc = fiona.open(basins)\n",
    "    shapefile_record = fc.next()\n",
    "    for shapefile_record in fc:\n",
    "        translated[int(shapefile_record['properties']['cat'])] = int(shapefile_record['id'])\n",
    "    return translated\n",
    "\n",
    "# translate REW IDs to feature IDs\n",
    "def translate_to_rew_id(parent_dir):\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "    translated = {}\n",
    "\n",
    "    fc = fiona.open(basins)\n",
    "    shapefile_record = fc.next()\n",
    "    for shapefile_record in fc:\n",
    "        translated[int(shapefile_record['id'])] = int(shapefile_record['properties']['cat'])\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have all the pieces to compute PET. We'll use the Priestley-Taylor implementation from the `evap` module. We assume that the soil heat flux is zero at the daily timescale. Once all forcing timeseries have been computed, we save to `rew_forcing.p` in the `model_data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Eqn 50 From Allen (1998) to get solar radiation from max/min temp difference\n",
    "kRs = 0.18\n",
    "\n",
    "# centroids of basins\n",
    "centroids = pd.read_csv( os.path.join(parent_dir, 'raw_data','basins_centroids', 'points.csv') )\n",
    "\n",
    "for rew_id in rew_dfs_dict.keys():\n",
    "    doy = [rng[i].timetuple().tm_yday for i in range(len(rng))]\n",
    "    N, Rext = meteo.sun_NR(doy, float(centroids['lat'].loc[centroids['cat']==rew_id]))\n",
    "    Rext_MJ = Rext/(10.0**6)\n",
    "    tmax = rew_dfs_dict[rew_id]['tmax']\n",
    "    tmin = rew_dfs_dict[rew_id]['tmin']\n",
    "    tmean = rew_dfs_dict[rew_id]['tmean']\n",
    "    Rs = kRs*np.sqrt(tmax-tmin)*Rext\n",
    "    pet = evap.hargreaves(tmin, tmax, tmean, Rext_MJ)\n",
    "    pet = pet.ffill()\n",
    "    rew_dfs_dict[rew_id]['pet'] = pet/10.0  #into units of cm/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elder PET sums\n",
      "For 2015 701.08 mm\n",
      "For 2017 683.71 mm\n",
      "Dry PET sums\n",
      "For 2015 746.48 mm\n",
      "For 2017 727.36 mm\n"
     ]
    }
   ],
   "source": [
    "print('Elder PET sums')\n",
    "print('For 2015 %.2f mm'%(10*rew_dfs_dict[1].pet.loc['5-2015':'9-2015'].sum()))\n",
    "print('For 2017 %.2f mm'%(10*rew_dfs_dict[1].pet.loc['5-2017':'9-2017'].sum()))\n",
    "\n",
    "print('Dry PET sums')\n",
    "print('For 2015 %.2f mm'%(10*rew_dfs_dict[2].pet.loc['5-2015':'9-2015'].sum()))\n",
    "print('For 2017 %.2f mm'%(10*rew_dfs_dict[2].pet.loc['5-2017':'9-2017'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data to climate groups\n",
    "\n",
    "The model itself requires forcing data to be indexed by climate groups, not REW id's. For this tutorial, each REW comprises its own climate group. However, it may be that the user would want to lump REWs with similar climates into groups, within which each REW is forced with identical data. Here, we demonstrate a straightforward aggregation procedure that would take the average of each forcing variable across the REWs in each climate group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the forcing dictionary must have climate_group id's as keys. \n",
    "# for the time being, we will merge REWs within each climate group\n",
    "# using the mean of the forcings for all REWs in the climate group. \n",
    "rew_config = pickle.load( open( os.path.join(parent_dir,'model_data','rew_config.p'), \"rb\" ) )\n",
    "climate_group_forcing = {}\n",
    "for climate_group in set([rew_config[i]['climate_group'] for i in rew_config.keys()]):\n",
    "    rew_ids_in_climate_group = [i for i in rew_config.keys() if rew_config[i]['climate_group']==climate_group]\n",
    "    rew_dfs  = [rew_dfs_dict[rew_id] for rew_id in rew_ids_in_climate_group]\n",
    "    df = pd.concat(rew_dfs)\n",
    "    df = df.groupby(df.index).mean()\n",
    "    climate_group_forcing[climate_group] = df\n",
    "\n",
    "    \n",
    "# Now save the dictionary of dataframes, where the keys are climate groups and each entry is a pandas dataframe \n",
    "# containing the forcings associated with that climate group. \n",
    "pickle.dump( climate_group_forcing, open( os.path.join(parent_dir,'model_data','climate_group_forcing.p'), \"wb\" ) )\n",
    "\n",
    "# Also save the original REW climatic forcing dataframe. This will be used for the temperature model\n",
    "pickle.dump( rew_dfs_dict, open( os.path.join(parent_dir,'model_data','rew_forcing.p'), \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2k_model]",
   "language": "python",
   "name": "conda-env-py2k_model-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
