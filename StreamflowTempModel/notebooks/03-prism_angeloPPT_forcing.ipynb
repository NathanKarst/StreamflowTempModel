{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Forcing Extraction Tutorial\n",
    "\n",
    "Using PRISM for temperature and dewpoint, but use Angelo data for precipitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import gdal\n",
    "from functools import reduce   \n",
    "import os\n",
    "import numpy as np\n",
    "import fiona\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "from os.path import dirname\n",
    "import glob\n",
    "import sys\n",
    "import pickle\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "parent_dir = dirname(dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','lib'))\n",
    "rew_config = pd.read_pickle(os.path.join(parent_dir,'model_data','rew_config.p'))\n",
    "import meteolib as meteo\n",
    "import evaplib as evap\n",
    "model_config = pd.read_pickle(os.path.join(parent_dir,'model_data','model_config.p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We first need to specify the geographic areas of interest, comprised of the collection of REW sub-basins. We'll assume that these regions are specified via shapefiles located in the `raw_data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "except RuntimeError:\n",
    "    print('Cannot find basins shapefile. Please make sure basins shapefile is located in \\n the model directory under /raw_data/basins_poly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For simplicity, we'll assume that the forcing along the spatial extent of each basin is constant and equal to the forcing observed at its centroid. We could also consider implementing zonal averaging or some other more faithful approximation, but this would be much more computationally intensive, and quite likely unnecessary for lower resolution climate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO CONSIDER CHANGING THIS TO JUST READ FROM THE CENTROIDS ALREADY GENERATED\n",
    "# ALSO NOTE: new projection does not return lat/long points for xy coordinates\n",
    "fc = fiona.open(basins)\n",
    "# shapefile_record = fc.next()\n",
    "shapefile_record = next(iter(fc))\n",
    "\n",
    "\n",
    "pos_dict={}\n",
    "for shapefile_record in fc:\n",
    "    shape = shapely.geometry.asShape(shapefile_record['geometry'])\n",
    "    long_point = shape.centroid.coords.xy[0][0]\n",
    "    lat_point = shape.centroid.coords.xy[1][0]\n",
    "    pos = (long_point, lat_point)\n",
    "    pos_dict[int(shapefile_record['properties']['cat'])]=pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With the list of centroids in hand, we can begin unpacking the forcing data. Here we'll focus on three time series: daily precipitation (`ppt`), mean daily temperature (`tmean`), and mean daily dew point (`tdmean`). \n",
    "\n",
    "Our main data structure here will be a dictionary whose keys are forcing data types and whose values are themselves dictionaries. These inner dictionaries have REW IDs as keys, and forcing data at particular instances of time as values.\n",
    "\n",
    "We track time in a separate dictionary, again keyed based on forcing data. The values here are lists of dates at which each type of forcing data was observed. \n",
    "\n",
    "Note: we assume that each forcing time series is stored exactly as would be downloaded from the PRISM download page. A simple bash script for downloading daily PRISM data can be found [HERE](https://github.com/daviddralle/downloadPrism). The files must be unzipped after download. For the purposes of the model, the data should be stored in the `raw_data` folder in a foldername equal to the variable name, for instance precipitation (`ppt`) is in `raw_data/ppt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmean data...\n",
      "1981 tmean data processed.\n",
      "1982 tmean data processed.\n",
      "1983 tmean data processed.\n",
      "1984 tmean data processed.\n",
      "1985 tmean data processed.\n",
      "1986 tmean data processed.\n",
      "1987 tmean data processed.\n",
      "1988 tmean data processed.\n",
      "1989 tmean data processed.\n",
      "1990 tmean data processed.\n",
      "1991 tmean data processed.\n",
      "1992 tmean data processed.\n",
      "1993 tmean data processed.\n",
      "1994 tmean data processed.\n",
      "1995 tmean data processed.\n",
      "1996 tmean data processed.\n",
      "1997 tmean data processed.\n",
      "1998 tmean data processed.\n",
      "1999 tmean data processed.\n",
      "2000 tmean data processed.\n",
      "2001 tmean data processed.\n",
      "2002 tmean data processed.\n",
      "2003 tmean data processed.\n",
      "2004 tmean data processed.\n",
      "2005 tmean data processed.\n",
      "2006 tmean data processed.\n",
      "2007 tmean data processed.\n",
      "2008 tmean data processed.\n",
      "2009 tmean data processed.\n",
      "2010 tmean data processed.\n",
      "2011 tmean data processed.\n",
      "2012 tmean data processed.\n",
      "2013 tmean data processed.\n",
      "2014 tmean data processed.\n",
      "2015 tmean data processed.\n",
      "2016 tmean data processed.\n",
      "2017 tmean data processed.\n",
      "Extracting tdmean data...\n",
      "1981 tdmean data processed.\n",
      "1982 tdmean data processed.\n",
      "1983 tdmean data processed.\n",
      "1984 tdmean data processed.\n",
      "1985 tdmean data processed.\n",
      "1986 tdmean data processed.\n",
      "1987 tdmean data processed.\n",
      "1988 tdmean data processed.\n",
      "1989 tdmean data processed.\n",
      "1990 tdmean data processed.\n",
      "1991 tdmean data processed.\n",
      "1992 tdmean data processed.\n",
      "1993 tdmean data processed.\n",
      "1994 tdmean data processed.\n",
      "1995 tdmean data processed.\n",
      "1996 tdmean data processed.\n",
      "1997 tdmean data processed.\n",
      "1998 tdmean data processed.\n",
      "1999 tdmean data processed.\n",
      "2000 tdmean data processed.\n",
      "2001 tdmean data processed.\n",
      "2002 tdmean data processed.\n",
      "2003 tdmean data processed.\n",
      "2004 tdmean data processed.\n",
      "2005 tdmean data processed.\n",
      "2006 tdmean data processed.\n",
      "2007 tdmean data processed.\n",
      "2008 tdmean data processed.\n",
      "2009 tdmean data processed.\n",
      "2010 tdmean data processed.\n",
      "2011 tdmean data processed.\n",
      "2012 tdmean data processed.\n",
      "2013 tdmean data processed.\n",
      "2014 tdmean data processed.\n",
      "2015 tdmean data processed.\n",
      "2016 tdmean data processed.\n",
      "2017 tdmean data processed.\n",
      "Extracting tmin data...\n",
      "1981 tmin data processed.\n",
      "1982 tmin data processed.\n",
      "1983 tmin data processed.\n",
      "1984 tmin data processed.\n",
      "1985 tmin data processed.\n",
      "1986 tmin data processed.\n",
      "1987 tmin data processed.\n",
      "1988 tmin data processed.\n",
      "1989 tmin data processed.\n",
      "1990 tmin data processed.\n",
      "1991 tmin data processed.\n",
      "1992 tmin data processed.\n",
      "1993 tmin data processed.\n",
      "1994 tmin data processed.\n",
      "1995 tmin data processed.\n",
      "1996 tmin data processed.\n",
      "1997 tmin data processed.\n",
      "1998 tmin data processed.\n",
      "1999 tmin data processed.\n",
      "2000 tmin data processed.\n",
      "2001 tmin data processed.\n",
      "2002 tmin data processed.\n",
      "2003 tmin data processed.\n",
      "2004 tmin data processed.\n",
      "2005 tmin data processed.\n",
      "2006 tmin data processed.\n",
      "2007 tmin data processed.\n",
      "2008 tmin data processed.\n",
      "2009 tmin data processed.\n",
      "2010 tmin data processed.\n",
      "2011 tmin data processed.\n",
      "2012 tmin data processed.\n",
      "2013 tmin data processed.\n",
      "2014 tmin data processed.\n",
      "2015 tmin data processed.\n",
      "2016 tmin data processed.\n",
      "2017 tmin data processed.\n",
      "Extracting tmax data...\n",
      "1981 tmax data processed.\n",
      "1982 tmax data processed.\n",
      "1983 tmax data processed.\n",
      "1984 tmax data processed.\n",
      "1985 tmax data processed.\n",
      "1986 tmax data processed.\n",
      "1987 tmax data processed.\n",
      "1988 tmax data processed.\n",
      "1989 tmax data processed.\n",
      "1990 tmax data processed.\n",
      "1991 tmax data processed.\n",
      "1992 tmax data processed.\n",
      "1993 tmax data processed.\n",
      "1994 tmax data processed.\n",
      "1995 tmax data processed.\n",
      "1996 tmax data processed.\n",
      "1997 tmax data processed.\n",
      "1998 tmax data processed.\n",
      "1999 tmax data processed.\n",
      "2000 tmax data processed.\n",
      "2001 tmax data processed.\n",
      "2002 tmax data processed.\n",
      "2003 tmax data processed.\n",
      "2004 tmax data processed.\n",
      "2005 tmax data processed.\n",
      "2006 tmax data processed.\n",
      "2007 tmax data processed.\n",
      "2008 tmax data processed.\n",
      "2009 tmax data processed.\n",
      "2010 tmax data processed.\n",
      "2011 tmax data processed.\n",
      "2012 tmax data processed.\n",
      "2013 tmax data processed.\n",
      "2014 tmax data processed.\n",
      "2015 tmax data processed.\n",
      "2016 tmax data processed.\n",
      "2017 tmax data processed.\n"
     ]
    }
   ],
   "source": [
    "forcing_dict = {}\n",
    "dates_dict = {}\n",
    "start_date = model_config['start_date']\n",
    "stop_date = model_config['stop_date']\n",
    "prism_vars = ['tmean', 'tdmean', 'tmin', 'tmax']\n",
    "\n",
    "for prism_var in prism_vars:\n",
    "    print(\"Extracting \" + str(prism_var) + \" data...\")\n",
    "    years = os.listdir(os.path.join(parent_dir,'raw_data',prism_var))\n",
    "    years.sort()\n",
    "\n",
    "    # Initialize empty data structures -- a dict keyed on REW ID with empty lists as values, \n",
    "    # and an empty list that will hold dates at which prism_var was observed\n",
    "    vals_dict = {k:[] for k in pos_dict.keys()}\n",
    "    date_list=[]\n",
    "    \n",
    "    #for all years of forcing variable, load each day of raster data and extract to all REWs\n",
    "    for year in years:\n",
    "        try: \n",
    "            int(year)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # Assuming that data is in .tif format from PRISM.\n",
    "        # Data for each REW is extracted to REW centroid lat/lon. \n",
    "        raster_list = glob.glob(os.path.join(parent_dir,'raw_data',prism_var,year,'*.tif'))\n",
    "        for rast in raster_list:\n",
    "            thedate = pd.to_datetime(rast[-16:][:8]).date()\n",
    "            if (thedate>stop_date)|(thedate<start_date):\n",
    "                continue\n",
    "            date_list.append(thedate)\n",
    "            raster_file = os.path.join(parent_dir,'raw_data',prism_var,year,rast)\n",
    "            gdata = gdal.Open(raster_file)\n",
    "            gt = gdata.GetGeoTransform()\n",
    "            data = gdata.ReadAsArray().astype(np.float)\n",
    "            if len(np.shape(data))==3:\n",
    "                data = data[0,:,:]\n",
    "    \n",
    "            gdata = None\n",
    "            for rew_id in pos_dict.keys(): \n",
    "                pos = pos_dict[rew_id]\n",
    "                x = int((pos[0] - gt[0])/gt[1])\n",
    "                y = int((pos[1] - gt[3])/gt[5])\n",
    "                vals_dict[rew_id].append(data[y, x])\n",
    "                \n",
    "        print(str(year) + ' ' + prism_var + ' data processed.')               \n",
    "\n",
    "    forcing_dict[prism_var] = vals_dict\n",
    "    dates_dict[prism_var] = date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before using the data for modeling, we need to ensure that the observation dates for each forcing time series align. We leverage Python's sets to make sure that all the datelists are in fact the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates match!\n"
     ]
    }
   ],
   "source": [
    "union_all = list(reduce(set.union, [set(item) for item in list(dates_dict.values())]))\n",
    "if False in [len(union_all)==len(item) for item in list(dates_dict.values())]:\n",
    "    print('Forcing data dates do not all match, please check raw data!')\n",
    "else:\n",
    "    print('Dates match!')\n",
    "    union_all.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In general, we will house all timeseries data in Pandas dataframes in order to leverage some very nice resampling functionality. For now, we'll just initialize one empty dataframe for each REW. Each of these dataframes will be indexed by date and will have one column for each forcing timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rng = pd.date_range(union_all[0],union_all[-1])\n",
    "rew_dfs_dict = {k:pd.DataFrame(data=None,index=rng) for k in pos_dict.keys()}\n",
    "\n",
    "for prism_var in prism_vars:\n",
    "    for rew_id in pos_dict.keys():\n",
    "        rew_dfs_dict[rew_id][prism_var] = np.array(forcing_dict[prism_var][rew_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Add precipitation data to rew dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load daily precip from Angelo database, stored in raw_data folder\n",
    "angelo_ppt = pd.read_pickle(os.path.join(parent_dir, 'raw_data', 'elder_data', 'elder_ppt.p'))\n",
    "angelo_ppt = angelo_ppt.ffill()\n",
    "\n",
    "# Create precipitation columns in each forcing dictionary\n",
    "for rew_id in pos_dict.keys():\n",
    "    rew_dfs_dict[rew_id]['ppt'] = angelo_ppt.loc[rew_dfs_dict[rew_id].index]\n",
    "        \n",
    "# Remove any intercepted rainfall \n",
    "for rew_id in rew_dfs_dict.keys():\n",
    "    maxvals = rew_dfs_dict[rew_id].apply(lambda row: np.max([row['ppt'] - rew_config[rew_id]['interception_factor'], 0]),axis=1)\n",
    "    rew_dfs_dict[rew_id]['ppt'] = maxvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The last remaining forcing timeseries required by the model presented in this tutorial is daily average potential evapotranspiration (in units of cm/day). Here, we'll use the Priestly-Taylor equation to compute this forcing.   Before we get started, we need to build some helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compute relative humidity using temp and dewpoint\n",
    "def get_rh(tmean, tdmean):\n",
    "    a = 17.625\n",
    "    b = 243.04\n",
    "    return 100*np.exp(a*tdmean/(b+tdmean))/np.exp(a*tmean/(b+tmean))\n",
    "\n",
    "def get_ea(tdmean):\n",
    "    return 0.6108*np.exp((17.27*tdmean)/(tdmean+237.3))\n",
    "\n",
    "def get_rew_pressures(rew_elevations):\n",
    "    #elevations in meters, output pressure in Pa\n",
    "    #using simple elevation/pressure relationship\n",
    "    rew_pressures = {}\n",
    "    for key in rew_elevations.keys():\n",
    "        rew_pressures[key] = 1000*101.325*((293-0.0065*rew_elevations[key])/293)**5.26\n",
    "    return rew_pressures\n",
    "\n",
    "# # translate feature IDs to REW IDs\n",
    "# def translate_to_fid(parent_dir):\n",
    "#     basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "#     translated = {}\n",
    "#     fc = fiona.open(basins)\n",
    "#     shapefile_record = fc.next()\n",
    "#     for shapefile_record in fc:\n",
    "#         translated[int(shapefile_record['properties']['cat'])] = int(shapefile_record['id'])\n",
    "#     return translated\n",
    "\n",
    "# # translate REW IDs to feature IDs\n",
    "# def translate_to_rew_id(parent_dir):\n",
    "#     basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "#     translated = {}\n",
    "\n",
    "#     fc = fiona.open(basins)\n",
    "#     shapefile_record = fc.next()\n",
    "#     for shapefile_record in fc:\n",
    "#         translated[int(shapefile_record['id'])] = int(shapefile_record['properties']['cat'])\n",
    "#     return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We finally have all the pieces to compute PET. We'll use the Priestley-Taylor implementation from the `evap` module. We assume that the soil heat flux is zero at the daily timescale. Once all forcing timeseries have been computed, we save to `rew_forcing.p` in the `model_data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for now, just assign 0.5 m/s to wind - Downloaded ncep re-analysis data\n",
    "# Using Eqn 50 From Allen (1998) to get solar radiation from max/min temp difference\n",
    "kRs = 0.18\n",
    "\n",
    "# centroids of basins\n",
    "centroids = pd.read_csv( os.path.join(parent_dir, 'raw_data','basins_centroids', 'points.csv') )\n",
    "\n",
    "# get forcing dates\n",
    "rng = rew_dfs_dict[rew_id].ppt.index\n",
    "\n",
    "for rew_id in rew_dfs_dict.keys():\n",
    "    doy = [rng[i].timetuple().tm_yday for i in range(len(rng))]\n",
    "    N, Rext = meteo.sun_NR(doy, float(centroids['lat'].loc[centroids['cat']==rew_id]))\n",
    "    Rext_MJ = Rext/(10.0**6)\n",
    "    tmax = rew_dfs_dict[rew_id]['tmax']\n",
    "    tmin = rew_dfs_dict[rew_id]['tmin']\n",
    "    tmean = rew_dfs_dict[rew_id]['tmean']\n",
    "    Rs = kRs*np.sqrt(tmax-tmin)*Rext\n",
    "    pet = evap.hargreaves(tmin, tmax, tmean, Rext_MJ)\n",
    "    pet = pet.ffill()\n",
    "    rew_dfs_dict[rew_id]['pet'] = pet/10.0  #into units of cm/day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Aggregate data to climate groups\n",
    "\n",
    "The model itself requires forcing data to be indexed by climate groups, not REW id's. For this tutorial, each REW comprises its own climate group. However, it may be that the user would want to lump REWs with similar climates into groups, within which each REW is forced with identical data. Here, we demonstrate a straightforward aggregation procedure that would take the average of each forcing variable across the REWs in each climate group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the forcing dictionary must have climate_group id's as keys. \n",
    "# for the time being, we will merge REWs within each climate group\n",
    "# using the mean of the forcings for all REWs in the climate group. \n",
    "rew_config = pickle.load( open( os.path.join(parent_dir,'model_data','rew_config.p'), \"rb\" ) )\n",
    "climate_group_forcing = {}\n",
    "for climate_group in set([rew_config[i]['climate_group'] for i in rew_config.keys()]):\n",
    "    rew_ids_in_climate_group = [i for i in rew_config.keys() if rew_config[i]['climate_group']==climate_group]\n",
    "    rew_dfs  = [rew_dfs_dict[rew_id] for rew_id in rew_ids_in_climate_group]\n",
    "    df = pd.concat(rew_dfs)\n",
    "    df = df.groupby(df.index).mean()\n",
    "    climate_group_forcing[climate_group] = df\n",
    "\n",
    "# Now save the dictionary of dataframes, where the keys are climate groups and each entry is a pandas dataframe \n",
    "# containing the forcings associated with that climate group. \n",
    "pickle.dump( climate_group_forcing, open( os.path.join(parent_dir,'model_data','climate_group_forcing.p'), \"wb\" ) )\n",
    "\n",
    "# Also save the original REW climatic forcing dataframe. This will be used for the temperature model\n",
    "pickle.dump( rew_dfs_dict, open( os.path.join(parent_dir,'model_data','rew_forcing.p'), \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3k",
   "language": "python",
   "name": "py3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
