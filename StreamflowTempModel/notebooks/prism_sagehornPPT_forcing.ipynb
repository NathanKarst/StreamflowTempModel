{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcing Extraction Tutorial\n",
    "\n",
    "Using PRISM for temperature and dewpoint, but use Angelo data for precipitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import gdal\n",
    "import os\n",
    "import numpy as np\n",
    "import fiona\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "from os.path import dirname\n",
    "import glob\n",
    "import sys\n",
    "import pickle\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "parent_dir = dirname(dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','lib'))\n",
    "rew_config = pickle.load( open( os.path.join(parent_dir,'model_data','rew_config.p'), \"rb\" ) )\n",
    "import zonal_stats as zs\n",
    "import meteolib as meteo\n",
    "import evaplib as evap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to specify the geographic areas of interest, comprised of the collection of REW sub-basins. We'll assume that these regions are specified via shapefiles located in the `raw_data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "except RuntimeError:\n",
    "    print 'Cannot find basins shapefile. Please make sure basins shapefile is located in \\n the model directory under /raw_data/basins_poly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we'll assume that the forcing along the spatial extent of each basin is constant and equal to the forcing observed at its centroid. We could also consider implementing zonal averaging or some other more faithful approximation, but this would be much more computationally intensive, and quite likely unnecessary for lower resolution climate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO CONSIDER CHANGING THIS TO JUST READ FROM THE CENTROIDS ALREADY GENERATED\n",
    "# ALSO NOTE: new projection does not return lat/long points for xy coordinates\n",
    "fc = fiona.open(basins)\n",
    "shapefile_record = fc.next()\n",
    "pos_dict={}\n",
    "for shapefile_record in fc:\n",
    "    shape = shapely.geometry.asShape(shapefile_record['geometry'])\n",
    "    long_point = shape.centroid.coords.xy[0][0]\n",
    "    lat_point = shape.centroid.coords.xy[1][0]\n",
    "    pos = (long_point, lat_point)\n",
    "    pos_dict[int(shapefile_record['properties']['cat'])]=pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the list of centroids in hand, we can begin unpacking the forcing data. Here we'll focus on three time series: daily precipitation (`ppt`), mean daily temperature (`tmean`), and mean daily dew point (`tdmean`). \n",
    "\n",
    "Our main data structure here will be a dictionary whose keys are forcing data types and whose values are themselves dictionaries. These inner dictionaries have REW IDs as keys, and forcing data at particular instances of time as values.\n",
    "\n",
    "We track time in a separate dictionary, again keyed based on forcing data. The values here are lists of dates at which each type of forcing data was observed. \n",
    "\n",
    "Note: we assume that each forcing time series is stored exactly as would be downloaded from the PRISM download page. A simple bash script for downloading daily PRISM data can be found [HERE](https://github.com/daviddralle/downloadPrism). The files must be unzipped after download. For the purposes of the model, the data should be stored in the `raw_data` folder in a foldername equal to the variable name, for instance precipitation (`ppt`) is in `raw_data/ppt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmean data...\n",
      "2012 tmean data processed.\n",
      "2013 tmean data processed.\n",
      "2014 tmean data processed.\n",
      "2015 tmean data processed.\n",
      "2016 tmean data processed.\n",
      "Extracting tdmean data...\n",
      "2012 tdmean data processed.\n",
      "2013 tdmean data processed.\n",
      "2014 tdmean data processed.\n",
      "2015 tdmean data processed.\n",
      "2016 tdmean data processed.\n",
      "Extracting tmin data...\n",
      "2012 tmin data processed.\n",
      "2013 tmin data processed.\n",
      "2014 tmin data processed.\n",
      "2015 tmin data processed.\n",
      "2016 tmin data processed.\n",
      "Extracting tmax data...\n",
      "2012 tmax data processed.\n",
      "2013 tmax data processed.\n",
      "2014 tmax data processed.\n",
      "2015 tmax data processed.\n",
      "2016 tmax data processed.\n"
     ]
    }
   ],
   "source": [
    "forcing_dict = {}\n",
    "dates_dict = {}\n",
    "\n",
    "prism_vars = ['tmean', 'tdmean', 'tmin', 'tmax']\n",
    "\n",
    "for prism_var in prism_vars:\n",
    "    print \"Extracting \" + str(prism_var) + \" data...\"\n",
    "    years = os.listdir(os.path.join(parent_dir,'raw_data',prism_var))\n",
    "\n",
    "    # Initialize empty data structures -- a dict keyed on REW ID with empty lists as values, \n",
    "    # and an empty list that will hold dates at which prism_var was observed\n",
    "    vals_dict = {k:[] for k in pos_dict.keys()}\n",
    "    date_list=[]\n",
    "    \n",
    "    #for all years of forcing variable, load each day of raster data and extract to all REWs\n",
    "    for year in years:\n",
    "        try: \n",
    "            int(year)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        # Assuming that data is in .tif format from PRISM.\n",
    "        # Data for each REW is extracted to REW centroid lat/long. \n",
    "        raster_list = glob.glob(os.path.join(parent_dir,'raw_data',prism_var,year,'*.tif'))\n",
    "        for rast in raster_list:\n",
    "            date_list.append(rast[-16:][:8])\n",
    "            raster_file = os.path.join(parent_dir,'raw_data',prism_var,year,rast)\n",
    "            gdata = gdal.Open(raster_file)\n",
    "            gt = gdata.GetGeoTransform()\n",
    "            data = gdata.ReadAsArray().astype(np.float)\n",
    "            gdata = None\n",
    "            for rew_id in pos_dict.keys(): \n",
    "                pos = pos_dict[rew_id]\n",
    "                x = int((pos[0] - gt[0])/gt[1])\n",
    "                y = int((pos[1] - gt[3])/gt[5])\n",
    "                vals_dict[rew_id].append(data[y, x])\n",
    "                \n",
    "        print str(year) + ' ' + prism_var + ' data processed.'                \n",
    "\n",
    "    forcing_dict[prism_var] = vals_dict\n",
    "    dates_dict[prism_var] = date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the data for modeling, we need to ensure that the observation dates for each forcing time series align. We leverage Python's sets to make sure that all the datelists are in fact the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates match!\n"
     ]
    }
   ],
   "source": [
    "unique_dates = [list(i) for i in set(tuple(i) for i in dates_dict.values())]\n",
    "if len(unique_dates)>1:\n",
    "    print 'Forcing data dates do not all match, please check raw data!'\n",
    "else:\n",
    "    print 'Dates match!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we will house all timeseries data in Pandas dataframes in order to leverage some very nice resampling functionality. For now, we'll just initialize one empty dataframe for each REW. Each of these dataframes will be indexed by date and will have one column for each forcing timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = pd.date_range(unique_dates[0][0],unique_dates[0][-1])\n",
    "rew_dfs_dict = {k:pd.DataFrame(data=None,index=rng) for k in pos_dict.keys()}\n",
    "\n",
    "for prism_var in prism_vars:\n",
    "    for rew_id in pos_dict.keys():\n",
    "        rew_dfs_dict[rew_id][prism_var] = np.array(forcing_dict[prism_var][rew_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add precipitation data to rew dataframes, convert to cm/day from mm/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load daily precip from Angelo (sagehorn) database, stored in raw_data folder\n",
    "sagehorn_ppt = pickle.load( open(os.path.join(parent_dir, 'raw_data', 'sagehorn_data', 'sagehorn_ppt.p'),'rb') )\n",
    "sagehorn_ppt = sagehorn_ppt.ffill()\n",
    "\n",
    "# Create precipitation columns in each forcing dictionary\n",
    "for rew_id in pos_dict.keys():\n",
    "    rew_dfs_dict[rew_id]['ppt'] = sagehorn_ppt.loc[rew_dfs_dict[rew_id].index]/10.0\n",
    "        \n",
    "# Remove any intercepted rainfall \n",
    "for rew_id in rew_dfs_dict.keys():\n",
    "    maxvals = rew_dfs_dict[rew_id].apply(lambda row: np.max([row['ppt'] - rew_config[rew_id]['interception_factor'], 0]),axis=1)\n",
    "    rew_dfs_dict[rew_id].ix[:,'ppt'] = maxvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last remaining forcing timeseries required by the model presented in this tutorial is daily average potential evapotranspiration (in units of cm/day). Here, we'll use the Priestly-Taylor equation to compute this forcing.   Before we get started, we need to build some helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute relative humidity using temp and dewpoint\n",
    "def get_rh(tmean, tdmean):\n",
    "    a = 17.625\n",
    "    b = 243.04\n",
    "    return 100*np.exp(a*tdmean/(b+tdmean))/np.exp(a*tmean/(b+tmean))\n",
    "\n",
    "def get_ea(tdmean):\n",
    "    return 0.6108*np.exp((17.27*tdmean)/(tdmean+237.3))\n",
    "\n",
    "def get_rew_pressures(rew_elevations):\n",
    "    #elevations in meters, output pressure in Pa\n",
    "    #using simple elevation/pressure relationship\n",
    "    rew_pressures = {}\n",
    "    for key in rew_elevations.keys():\n",
    "        rew_pressures[key] = 1000*101.325*((293-0.0065*rew_elevations[key])/293)**5.26\n",
    "\n",
    "    return rew_pressures\n",
    "\n",
    "# translate feature IDs to REW IDs\n",
    "def translate_to_fid(parent_dir):\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "    translated = {}\n",
    "\n",
    "    fc = fiona.open(basins)\n",
    "    shapefile_record = fc.next()\n",
    "    for shapefile_record in fc:\n",
    "        translated[int(shapefile_record['properties']['cat'])] = int(shapefile_record['id'])\n",
    "    return translated\n",
    "\n",
    "# translate REW IDs to feature IDs\n",
    "def translate_to_rew_id(parent_dir):\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "    translated = {}\n",
    "\n",
    "    fc = fiona.open(basins)\n",
    "    shapefile_record = fc.next()\n",
    "    for shapefile_record in fc:\n",
    "        translated[int(shapefile_record['id'])] = int(shapefile_record['properties']['cat'])\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have all the pieces to compute PET. We'll use the Priestley-Taylor implementation from the `evap` module. We assume that the soil heat flux is zero at the daily timescale. Once all forcing timeseries have been computed, we save to `rew_forcing.p` in the `model_data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for now, just assign 0.5 m/s to wind - Downloaded ncep re-analysis data\n",
    "# Using Eqn 50 From Allen (1998) to get solar radiation from max/min temp difference\n",
    "kRs = 0.18\n",
    "\n",
    "# centroids of basins\n",
    "centroids = pd.read_csv( os.path.join(parent_dir, 'raw_data','basins_centroids', 'points.csv') )\n",
    "\n",
    "# get forcing dates\n",
    "rng = rew_dfs_dict[rew_id].ppt.index\n",
    "\n",
    "for rew_id in rew_dfs_dict.keys():\n",
    "    doy = [rng[i].timetuple().tm_yday for i in range(len(rng))]\n",
    "    N, Rext = meteo.sun_NR(doy, float(centroids['lat'].loc[centroids['cat']==rew_id]))\n",
    "    Rext_MJ = Rext/(10.0**6)\n",
    "    tmax = rew_dfs_dict[rew_id]['tmax']\n",
    "    tmin = rew_dfs_dict[rew_id]['tmin']\n",
    "    tmean = rew_dfs_dict[rew_id]['tmean']\n",
    "    Rs = kRs*np.sqrt(tmax-tmin)*Rext\n",
    "    pet = evap.hargreaves(tmin, tmax, tmean, Rext_MJ)\n",
    "    pet = pet.ffill()\n",
    "    rew_dfs_dict[rew_id]['pet'] = pet/10.0  #into units of cm/day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data to climate groups\n",
    "\n",
    "The model itself requires forcing data to be indexed by climate groups, not REW id's. For this tutorial, each REW comprises its own climate group. However, it may be that the user would want to lump REWs with similar climates into groups, within which each REW is forced with identical data. Here, we demonstrate a straightforward aggregation procedure that would take the average of each forcing variable across the REWs in each climate group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the forcing dictionary must have climate_group id's as keys. \n",
    "# for the time being, we will merge REWs within each climate group\n",
    "# using the mean of the forcings for all REWs in the climate group. \n",
    "rew_config = pickle.load( open( os.path.join(parent_dir,'model_data','rew_config.p'), \"rb\" ) )\n",
    "climate_group_forcing = {}\n",
    "for climate_group in set([rew_config[i]['climate_group'] for i in rew_config.keys()]):\n",
    "    rew_ids_in_climate_group = [i for i in rew_config.keys() if rew_config[i]['climate_group']==climate_group]\n",
    "    rew_dfs  = [rew_dfs_dict[rew_id] for rew_id in rew_ids_in_climate_group]\n",
    "    df = pd.concat(rew_dfs)\n",
    "    df = df.groupby(df.index).mean()\n",
    "    climate_group_forcing[climate_group] = df\n",
    "\n",
    "    \n",
    "# Now save the dictionary of dataframes, where the keys are climate groups and each entry is a pandas dataframe \n",
    "# containing the forcings associated with that climate group. \n",
    "pickle.dump( climate_group_forcing, open( os.path.join(parent_dir,'model_data','climate_group_forcing.p'), \"wb\" ) )\n",
    "\n",
    "# Also save the original REW climatic forcing dataframe. This will be used for the temperature model\n",
    "pickle.dump( rew_dfs_dict, open( os.path.join(parent_dir,'model_data','rew_forcing.p'), \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2k_model]",
   "language": "python",
   "name": "conda-env-py2k_model-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
