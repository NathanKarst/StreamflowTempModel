{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcing Extraction Tutorial\n",
    "\n",
    "Use daymet data with Angelo precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import gdal\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import fiona\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "from os.path import dirname\n",
    "import glob\n",
    "import sys\n",
    "import pickle\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "parent_dir = dirname(dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','lib'))\n",
    "rew_config = pickle.load( open( os.path.join(parent_dir,'model_data','rew_config.p'), \"rb\" ) )\n",
    "import zonal_stats as zs\n",
    "import meteolib as meteo\n",
    "import evaplib as evap\n",
    "import daymetpy as dm\n",
    "from ulmo.nasa import daymet\n",
    "\n",
    "\n",
    "\n",
    "model_config = pickle.load( open( os.path.join(parent_dir, 'model_data', 'model_config.p'), 'rb'))\n",
    "start_date = model_config['start_date']\n",
    "stop_date = model_config['stop_date']\n",
    "spinup_date = model_config['spinup_date']\n",
    "Tmax = model_config['Tmax']\n",
    "dt = model_config['dt_hillslope']\n",
    "t = np.linspace(0,Tmax,np.ceil(Tmax/dt)+1)\n",
    "resample_freq_hillslope = model_config['resample_freq_hillslope']\n",
    "timestamps_hillslope = pd.date_range(start_date, stop_date, freq=resample_freq_hillslope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "angelo_ppt = pickle.load( open(os.path.join(parent_dir, 'raw_data', 'elder_data', 'elder_ppt.p'),'rb') )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to specify the geographic areas of interest, comprised of the collection of REW sub-basins. We'll assume that these regions are specified via shapefiles located in the `raw_data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "except RuntimeError:\n",
    "    print 'Cannot find basins shapefile. Please make sure basins shapefile is located in \\n the model directory under /raw_data/basins_poly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we'll assume that the forcing along the spatial extent of each basin is constant and equal to the forcing observed at its centroid. We could also consider implementing zonal averaging or some other more faithful approximation, but this would be much more computationally intensive, and quite likely unnecessary for lower resolution climate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on REW 1 out of 3\n",
      "Working on REW 2 out of 3\n",
      "Working on REW 3 out of 3\n"
     ]
    }
   ],
   "source": [
    "# load daily precip from Angelo database, stored in raw_data folder\n",
    "angelo_ppt = pickle.load( open(os.path.join(parent_dir, 'raw_data', 'elder_data', 'elder_ppt.p'),'rb') )\n",
    "\n",
    "# Load lat/lon data for each REW\n",
    "points = pd.read_csv( os.path.join(parent_dir, 'raw_data', 'basins_centroids', 'points.csv') )\n",
    "points = points.sort_values('cat')\n",
    "rew_dfs_dict = {}\n",
    "for i,row in points.iterrows():\n",
    "    printstr = 'Working on REW ' + str(int(row['cat'])) + ' out of ' + str(np.max(points['cat']))\n",
    "    print(printstr)\n",
    "    df = dm.daymet_timeseries(lat=row.lat, lon=row.lon, start_year=start_date.year, end_year=stop_date.year, \n",
    "                      as_dataframe=True, download_dname=None, verbose=False)\n",
    "    \n",
    "    # daymet removes 12/31 on leap years; fill with 12/30 data\n",
    "    for year in set(df.index.year):\n",
    "        try:\n",
    "            assert(df.loc[datetime(year,12,31)])\n",
    "        except:\n",
    "            df.loc[datetime(year,12,31)] = df.loc[datetime(year,12,30)].copy()\n",
    "    df = df.sort_index()\n",
    "    df.columns = ['year','yday','dayl','ppt','srad','swe','tmax','tmin','vp']\n",
    "    df['es'] = meteo.es_calc(airtemp= np.mean([df['tmax'],df['tmin']]))\n",
    "    df['rh'] = df['vp']/df['es']\n",
    "    df['rh'].loc[df.rh>1.0] = 1.0\n",
    "    elevation = rew_config[int(row['cat'])]['elevation']\n",
    "    df['p'] = meteo.airpress_calc(elevation)\n",
    "    df = df.drop('ppt', 1)\n",
    "    df['ppt'] = angelo_ppt.loc[df.index]/10.0\n",
    "    df['tmean'] = (df['tmax'] + df['tmin'])/2.0\n",
    "    rew_dfs_dict[int(row['cat'])] = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REW 1 intercepted fraction = 0.06\n",
      "REW 2 intercepted fraction = 0.06\n",
      "REW 3 intercepted fraction = 0.06\n"
     ]
    }
   ],
   "source": [
    "# Remove any intercepted rainfall \n",
    "# only run this once! \n",
    "for rew_id in rew_dfs_dict.keys():\n",
    "    maxvals = rew_dfs_dict[rew_id].apply(lambda row: np.max([row['ppt'] - rew_config[rew_id]['interception_factor'], 0]),axis=1)\n",
    "    percent_intercepted = 1-np.sum(maxvals)/np.sum(rew_dfs_dict[rew_id].ix[:,'ppt'])\n",
    "    print('REW %s intercepted fraction = %.2f'%(str(rew_id), percent_intercepted))\n",
    "    rew_dfs_dict[rew_id].ix[:,'ppt'] = maxvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last remaining forcing timeseries required by the model presented in this tutorial is daily average potential evapotranspiration (in units of cm/day). Here, we'll use the Priestly-Taylor equation to compute this forcing.   Before we get started, we need to build some helper functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have all the pieces to compute PET. We'll use the Priestley-Taylor implementation from the `evap` module. We assume that the soil heat flux is zero at the daily timescale. Once all forcing timeseries have been computed, we save to `rew_forcing.p` in the `model_data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using Eqn 50 From Allen (1998) to get solar radiation from max/min temp difference\n",
    "kRs = 0.18\n",
    "\n",
    "# centroids of basins\n",
    "centroids = pd.read_csv( os.path.join(parent_dir, 'raw_data','basins_centroids', 'points.csv') )\n",
    "\n",
    "# get forcing dates\n",
    "rng = df.index\n",
    "\n",
    "for rew_id in rew_dfs_dict.keys():\n",
    "    doy = [rng[i].timetuple().tm_yday for i in range(len(rng))]\n",
    "    N, Rext = meteo.sun_NR(doy, float(centroids['lat'].loc[centroids['cat']==rew_id]))\n",
    "    Rext_MJ = Rext/(10.0**6)\n",
    "    tmax = rew_dfs_dict[rew_id]['tmax']\n",
    "    tmin = rew_dfs_dict[rew_id]['tmin']\n",
    "    tmean = rew_dfs_dict[rew_id]['tmean']\n",
    "    Rs = kRs*np.sqrt(tmax-tmin)*Rext\n",
    "    pet = evap.hargreaves(tmin, tmax, tmean, Rext_MJ)\n",
    "    pet = pet.ffill()\n",
    "    rew_dfs_dict[rew_id]['pet'] = pet/10.0  #into units of cm/day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data to climate groups\n",
    "\n",
    "The model itself requires forcing data to be indexed by climate groups, not REW id's. For this tutorial, each REW comprises its own climate group. However, it may be that the user would want to lump REWs with similar climates into groups, within which each REW is forced with identical data. Here, we demonstrate a straightforward aggregation procedure that would take the average of each forcing variable across the REWs in each climate group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the forcing dictionary must have climate_group id's as keys. \n",
    "# for the time being, we will merge REWs within each climate group\n",
    "# using the mean of the forcings for all REWs in the climate group. \n",
    "rew_config = pickle.load( open( os.path.join(parent_dir,'model_data','rew_config.p'), \"rb\" ) )\n",
    "climate_group_forcing = {}\n",
    "for climate_group in set([rew_config[i]['climate_group'] for i in rew_config.keys()]):\n",
    "    rew_ids_in_climate_group = [i for i in rew_config.keys() if rew_config[i]['climate_group']==climate_group]\n",
    "    rew_dfs  = [rew_dfs_dict[rew_id] for rew_id in rew_ids_in_climate_group]\n",
    "    df = pd.concat(rew_dfs)\n",
    "    df = df.groupby(df.index).mean()\n",
    "    climate_group_forcing[climate_group] = df\n",
    "\n",
    "    \n",
    "# Now save the dictionary of dataframes, where the keys are climate groups and each entry is a pandas dataframe \n",
    "# containing the forcings associated with that climate group. \n",
    "pickle.dump( climate_group_forcing, open( os.path.join(parent_dir,'model_data','climate_group_forcing.p'), \"wb\" ) )\n",
    "\n",
    "# Also save the original REW climatic forcing dataframe. This will be used for the temperature model\n",
    "pickle.dump( rew_dfs_dict, open( os.path.join(parent_dir,'model_data','rew_forcing.p'), \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2k_model]",
   "language": "python",
   "name": "conda-env-py2k_model-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
