{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Calibration Tutorial\n",
    "\n",
    "This notebook outlines how to perform temperature model calibration for a selected subset of REWs. It is assumed that the selected subset is a unique sub-watershed of the full model watershed.\n",
    "\n",
    "Two files are required for temperature calibration:\n",
    "\n",
    "1. A shapefile corresponding to the sub-basin to be calibrated must be stored in `raw_data/watershed_poly`. \n",
    "2. Temperature data (in units of Celcius) stored in the `calibration_data` folder. This data must span at least the time period from `spinup_date` to `stop_date`. It is assumed that this data represents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from os.path import dirname\n",
    "parent_dir = dirname(dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','2_hillslope_discharge'))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','3_channel_routing'))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','4_temperature'))\n",
    "\n",
    "\n",
    "import random\n",
    "from vadoseZone import *\n",
    "import glob\n",
    "from groundwaterZone import *\n",
    "from REW import REW\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import mpld3\n",
    "import time\n",
    "import sys\n",
    "import shapely\n",
    "import fiona\n",
    "from pyDOE import *\n",
    "import folium\n",
    "from ast import literal_eval as make_tuple\n",
    "%matplotlib inline\n",
    "\n",
    "# Load config files, forcing file, and paramters for each group\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "sys.path.append(os.path.join(parent_dir, 'StreamflowTempModel', '1_data_preparation'))\n",
    "from prep import rew_params\n",
    "rew_params()\n",
    "\n",
    "rew_config = pickle.load( open( os.path.join(parent_dir,'model_data','rew_config.p'), \"rb\" ) )\n",
    "climate_group_forcing = pickle.load( open( os.path.join(parent_dir,'model_data','climate_group_forcing.p'), \"rb\" ) )\n",
    "model_config = pickle.load( open( os.path.join(parent_dir, 'model_data', 'model_config.p'), 'rb'))\n",
    "temperature_params = pickle.load( open( os.path.join(parent_dir, 'model_data', 'temperature_params.p'), 'rb'))\n",
    "hill_groups = pickle.load( open( os.path.join(parent_dir,'model_data','solved_hillslope_discharge.p'), \"rb\" ) )\n",
    "solved_channel_routing = pickle.load( open( os.path.join(parent_dir,'model_data','solved_channel_routing.p'), \"rb\" ) )\n",
    "channel_params = pickle.load( open( os.path.join(parent_dir,'model_data','channel_params.p'), \"rb\" ))\n",
    "temperature_param_ranges = pickle.load( open( os.path.join(parent_dir, 'model_data', 'temperature_param_ranges.p'), 'rb'))\n",
    "\n",
    "#start/stop dates for running model  \n",
    "#spinup date is the date after start_date for which we assume model is finished spinning up         \n",
    "start_date = model_config['start_date']\n",
    "stop_date = model_config['stop_date']\n",
    "spinup_date = model_config['spinup_date']\n",
    "Tmax = model_config['Tmax']\n",
    "dt = model_config['dt_temperature']\n",
    "t = model_config['t_temperature']\n",
    "resample_freq_channel = model_config['resample_freq_channel']\n",
    "resample_freq_hillslope = model_config['resample_freq_hillslope']\n",
    "resample_freq_temperature = model_config['resample_freq_temperature']\n",
    "timestamps_hillslope = model_config['timestamps_hillslope']\n",
    "timestamps_channel = model_config['timestamps_channel']\n",
    "timestamps_temperature = model_config['timestamps_temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get REWs located within calibration sub-watershed \n",
    "\n",
    "Here, we use the representative REW points to determine which REWs are located within the sub-watershed that we are calibrating. We want to make sure to run the model only for the REWs that are relevant for calibration. If no REWs are contained within the sub-watershed, the REW in which the sub-watershed is located will be calibrated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWs [1, 2, 3] are located within the calibration sub-watershed\n"
     ]
    }
   ],
   "source": [
    "# must specify prefix of .shp file corresponding to subwatershed\n",
    "# the .shp file must be located within the raw_data/watershed_poly folder\n",
    "subwatershed_name = 'elder'\n",
    "\n",
    "shapefile_path = os.path.join(parent_dir, 'raw_data','watershed_poly', subwatershed_name + '.shp')\n",
    "points = pd.read_csv(os.path.join(parent_dir, 'raw_data','basins_centroids', 'points.csv')).set_index('cat')\n",
    "\n",
    "# get coordinate tuples corresponding to each REW\n",
    "for index, row in points.iterrows():\n",
    "    new_tuple = make_tuple(points['coords'].loc[index])\n",
    "    points['coords'].loc[index] = new_tuple\n",
    "\n",
    "# check to see which REWs fall within sub-watershed\n",
    "ids_in_subwatershed = []\n",
    "with fiona.open(shapefile_path) as fiona_collection:\n",
    "    for shapefile_record in fiona_collection:\n",
    "        # note: the shapefile record must be of type polygon, not multi-polygon\n",
    "        # i.e. the sub-watershed must be a single polygon\n",
    "        shape = shapely.geometry.Polygon( shapefile_record['geometry']['coordinates'][0] )\n",
    "\n",
    "        for index, row in points.iterrows(): \n",
    "            point =  shapely.geometry.Point(row[0][0], row[0][1]) # longitude, latitude\n",
    "            if shape.contains(point):\n",
    "                ids_in_subwatershed.append(index)\n",
    "\n",
    "ids_in_subwatershed = list(set(ids_in_subwatershed))\n",
    "\n",
    "# if no REWs found inside sub-watershed, \n",
    "# assume the sub-watershed is contained within a single REW. \n",
    "# Here, find the id of that REW\n",
    "if len(ids_in_subwatershed)==0:\n",
    "    subwatershed_shape = gp.GeoDataFrame.from_file(shapefile_path)\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "    with fiona.open(basins) as fiona_collection:\n",
    "        for shapefile_record in fiona_collection:\n",
    "            shape = shapely.geometry.Polygon( shapefile_record['geometry']['coordinates'][0] )\n",
    "            if shape.contains(subwatershed_shape['geometry'].loc[0].centroid):\n",
    "                ids_in_subwatershed.append(shapefile_record['properties']['cat'])\n",
    "    \n",
    "groups_to_calibrate = []\n",
    "for rew_id in ids_in_subwatershed:\n",
    "    groups_to_calibrate.append(rew_config[rew_id]['group'])\n",
    "    \n",
    "    \n",
    "print('REWs %s are located within the calibration sub-watershed' % str(ids_in_subwatershed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "\n",
    "Define a function which takes modeled data and observed data (as pandas dataframes) that must be minimized for calibration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def objective_function(modeled, observed):\n",
    "#     inds = ((modeled != 0) & (observed != 0))\n",
    "#     return np.abs(np.sum(np.abs(np.log(observed.loc[inds]) - np.log(modeled.loc[inds])))/np.sum(np.abs(np.log(observed.loc[inds]) - np.log(np.mean(observed.loc[inds])))))\n",
    "\n",
    "def objective_function(modeled, observed):\n",
    "    inds = ((modeled != 0) & (observed != 0))\n",
    "    return np.abs(np.sum((modeled[inds]-observed[inds])**2)/np.sum((observed[inds]-np.mean(observed[inds]))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k12': (0.05, 0.5), 'f': (0.1, 0.9), 's0R': (0, 0.4), 's0S': (0, 0.4), 'k2': (0.01, 0.1), 'k1': (0.05, 0.5), 'stR': (0.1, 0.9), 'stS': (0.1, 0.9), 'nR': (0.05, 0.2), 'nS': (0, 0.7)}\n"
     ]
    }
   ],
   "source": [
    "# specify the number of parameter sets to generate\n",
    "N = 100000\n",
    "\n",
    "parameter_realz = []\n",
    "for i in range(N):\n",
    "    temperature_params_current = {}\n",
    "    for w in parameter_group_params.keys():\n",
    "        parameter_group_params_current[w] = parameter_group_params[w].copy()\n",
    "        \n",
    "    for j, parameter_group in enumerate(parameter_ranges.keys()):\n",
    "            for k, parameter in enumerate(parameter_ranges[parameter_group].keys()):\n",
    "                new_value = random.random()*(parameter_ranges[parameter_group][parameter][1] - parameter_ranges[parameter_group][parameter][0]) + parameter_ranges[parameter_group][parameter][0]\n",
    "                parameter_group_params_current[parameter_group][parameter] = new_value\n",
    "    parameter_realz.append(parameter_group_params_current)\n",
    "\n",
    "print parameter_ranges[parameter_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# for each parameter realization\n",
    "solved_subwatersheds = []\n",
    "for i in range(N):\n",
    "    solved_groups = {}\n",
    "    parameter_group_params = {}\n",
    "    parameter_group_params = parameter_realz[i]\n",
    "\n",
    "    solved_group_hillslopes_dict = {}\n",
    "    for group_id in groups_to_calibrate:\n",
    "\n",
    "        parameter_group_id = group_id[0]\n",
    "        climate_group_id = group_id[1]\n",
    "\n",
    "        vz = parameter_group_params[parameter_group_id]['vz'](**parameter_group_params[parameter_group_id])\n",
    "        gz = parameter_group_params[parameter_group_id]['gz'](**parameter_group_params[parameter_group_id])    \n",
    "\n",
    "        rew = REW(vz, gz,  **{'pet':climate_group_forcing[climate_group_id].pet, 'ppt':climate_group_forcing[climate_group_id].ppt, 'aspect':90})\n",
    "\n",
    "        storageVZ    = np.zeros(np.size(t))\n",
    "        storageGZ     = np.zeros(np.size(t))\n",
    "        discharge       = np.zeros(np.size(t))\n",
    "        leakage         = np.zeros(np.size(t))\n",
    "        ET              = np.zeros(np.size(t))\n",
    "\n",
    "        # Resample pet and ppt to integration timestep\n",
    "        ppt = np.array(rew.ppt[start_date:stop_date].resample(resample_freq_hillslope).ffill())\n",
    "        pet = np.array(rew.pet[start_date:stop_date].resample(resample_freq_hillslope).ffill())\n",
    "\n",
    "        # Solve group hillslope\n",
    "        for l in range(len(t)):\n",
    "            rew.vz.update(dt,**{'ppt':ppt[l],'pet':pet[l]})\n",
    "            storageVZ[l] = rew.vz.storageVZ\n",
    "            leakage[l]      = rew.vz.leakage\n",
    "            ET[l]           = rew.vz.ET   \n",
    "            rew.gz.update(dt,**{'leakage':leakage[l]})\n",
    "            storageGZ[l] = rew.gz.storageGZ\n",
    "            discharge[l] = rew.gz.discharge\n",
    "\n",
    "        # resample as daily data\n",
    "        solved_groups[group_id] = pd.DataFrame({'discharge':discharge}, index=timestamps_hillslope).resample('D').mean()\n",
    "        \n",
    "    total_area = 0\n",
    "    for rew_id in ids_in_subwatershed:\n",
    "        total_area += rew_config[rew_id]['area_sqkm']\n",
    "    \n",
    "    name = str(i) + 'discharge'\n",
    "    solved_subwatershed = pd.DataFrame({name:np.zeros(len(timestamps_hillslope))}, index=timestamps_hillslope).resample('D').mean()\n",
    " \n",
    "    solved_subwatershed_array = np.zeros(int(len(solved_subwatershed)))\n",
    "    for rew_id in ids_in_subwatershed:\n",
    "        solved_subwatershed_array += rew_config[rew_id]['area_sqkm']/total_area*solved_groups[rew_config[rew_id]['group']]['discharge']\n",
    "    \n",
    "    solved_subwatershed[name] = solved_subwatershed_array\n",
    "    solved_subwatersheds.append(solved_subwatershed)\n",
    "    \n",
    "solved_subwatersheds = pd.concat(solved_subwatersheds,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model goodness of fit\n",
    "\n",
    "Here, each model run is compared to calibration data using the objective function as defined above. The user must specify the pickled dataframe with calibration runoff data in units of cm/day. Calibration data must be available at least from `spinup_date` to `stop_date`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calibration_data_filename = 'elder_runoff.p'\n",
    "\n",
    "calibration_data = pickle.load( open(os.path.join(parent_dir,'calibration_data',calibration_data_filename)))\n",
    "calibration_data = calibration_data[spinup_date:stop_date]\n",
    "col_name = calibration_data.columns[0]\n",
    "calibration_data.columns = ['calibration_data']\n",
    "df = pd.concat([calibration_data, solved_subwatersheds],1)\n",
    "\n",
    "nses = []\n",
    "for i in range(N):\n",
    "    name = str(i) + 'discharge'\n",
    "    if np.isfinite(np.sum(df[name][spinup_date:stop_date])):\n",
    "        if int(np.sum(df[name][spinup_date:stop_date])) == 0:\n",
    "            nses.append(-1)\n",
    "        else:\n",
    "            nses.append(objective_function( df['calibration_data'][spinup_date:stop_date], df[name][spinup_date:stop_date]))\n",
    "    else:\n",
    "        nses.append(-9999)\n",
    "        \n",
    "best_column = str(np.argmax(nses)) + 'discharge'\n",
    "i = int(best_column.replace('discharge',''))\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.plot(df[['calibration_data',best_column]][spinup_date:stop_date])\n",
    "plt.legend(['Calibration data', 'Best model run (NSE = %0.2f)' % np.max(nses)])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Runoff [cm/day]')\n",
    "plt.title( subwatershed_name + ' subwatershed calibration results')\n",
    "html = mpld3.fig_to_html(fig)\n",
    "\n",
    "print 'The best fit parameter set has an NSE of %0.2f' % (np.max(nses))\n",
    "\n",
    "for j, parameter_group in enumerate(parameter_ranges.keys()):\n",
    "    for k, parameter in enumerate(parameter_ranges[parameter_group].keys()):\n",
    "        new_value = parameter_realz[i][parameter_group][parameter]\n",
    "        print 'The best fit value for parameter %s in parameter group %s is %f' % (parameter, parameter_group, new_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: working in Folium, ALL projections must be converted to epsg='4326'\n",
    "watershed_name = 'sf_below_tenmile'\n",
    "subwatershed_name = 'elder'\n",
    "\n",
    "#Add watershed\n",
    "shapefile_path = os.path.join(parent_dir, 'raw_data','watershed_poly', watershed_name + '.shp')\n",
    "basins_shape = gp.GeoDataFrame.from_file(shapefile_path).to_crs(epsg='4326')\n",
    "basins_shape['coords'] = basins_shape['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "basins_shape['coords'] = [coords[0] for coords in basins_shape['coords']]\n",
    "basins = basins_shape.to_crs(epsg='4326').to_json()\n",
    "bounds = basins_shape.exterior.bounds\n",
    "\n",
    "mapa = folium.Map([basins_shape['coords'][0][1], basins_shape['coords'][0][0]],\n",
    "                  tiles='Stamen Terrain')\n",
    "\n",
    "folium.GeoJson(\n",
    "    basins,\n",
    "    style_function=lambda feature: {\n",
    "        'color' : '#00ff00',\n",
    "        'fillOpacity': .05\n",
    "        }\n",
    "    ).add_to(mapa)\n",
    "\n",
    "\n",
    "shapefile_path = os.path.join(parent_dir, 'raw_data','watershed_poly', subwatershed_name + '.shp')\n",
    "basins_shape = gp.GeoDataFrame.from_file(shapefile_path).to_crs(epsg='4326')\n",
    "basins_shape['coords'] = basins_shape['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "basins_shape['coords'] = [coords[0] for coords in basins_shape['coords']]\n",
    "basins = basins_shape.to_crs(epsg='4326').to_json()\n",
    "\n",
    "folium.GeoJson(\n",
    "    basins,\n",
    "    style_function=lambda feature: {\n",
    "        'color' : '#FF0000',\n",
    "        'opacity': 0.4\n",
    "        }\n",
    "    ).add_to(mapa)\n",
    "\n",
    "iframe = folium.element.IFrame(html=html, width=650, height=400)\n",
    "popup = folium.Popup(iframe, max_width=2650)\n",
    "folium.Marker([basins_shape['coords'][0][1], basins_shape['coords'][0][0]], popup=popup, icon=folium.Icon(color='red',icon='info-sign')).add_to(mapa)\n",
    "\n",
    "streams_path = glob.glob(os.path.join(parent_dir,'raw_data','streams_poly','*.shp'))[0]\n",
    "streams_shape = gp.GeoDataFrame.from_file(streams_path).to_crs(epsg='4326')\n",
    "streams = gp.GeoDataFrame(streams_shape['geometry'], crs=streams_shape.crs)\n",
    "streams['RGBA'] = '#0000ff'\n",
    "streams = streams.to_crs(epsg='4326').to_json()\n",
    "colors = []\n",
    "folium.GeoJson(\n",
    "    streams,\n",
    "    style_function=lambda feature: {\n",
    "        'color' : feature['properties']['RGBA'],\n",
    "        'weight' : 4, \n",
    "        'opacity': 1\n",
    "        }\n",
    "    ).add_to(mapa)\n",
    "\n",
    "\n",
    "calibration_output_name = subwatershed_name + '_calibration.html'\n",
    "mapa.fit_bounds([[ bounds['miny'].loc[0], bounds['minx'].loc[0]], [ bounds['maxy'].loc[0], bounds['maxx'].loc[0]]])\n",
    "mapa.save(os.path.join(parent_dir, 'calibration_output', calibration_output_name))\n",
    "mapa.save(os.path.join(calibration_output_name))\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
