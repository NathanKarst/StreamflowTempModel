{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hillslope Scale Calibration Tutorial\n",
    "\n",
    "This notebook outlines how to perform model calibration for a selected subset of REWs. It is assumed that the selected subset is a unique sub-watershed of the full model watershed. It is furthermore assumed that channel effects are not important at the scale of the sub-watershed, so that the model can be compared to streamflow data simply by up-scaling the hillslopes output. \n",
    "\n",
    "Two files are required for hillslope scale calibration:\n",
    "\n",
    "1. A shapefile corresponding to the sub-basin to be calibrated must be stored in `raw_data/watershed_poly`. \n",
    "2. Streamflow data (in units of cm/day) stored in the `calibration_data` folder. This must be gapless, daily streamflow data spanning at least the time period from `spinup_date` to `stop_date`. \n",
    "\n",
    "Full model calibration including channel transport is overviewed in the [Network Scale Calibration Tutorial](./network_scale_calibration.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daviddralle/anaconda2/envs/py2k_gis/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/Users/daviddralle/anaconda2/envs/py2k_gis/lib/python2.7/site-packages/osgeo/gdal.py:80: DeprecationWarning: gdal.py was placed in a namespace, it is now available as osgeo.gdal\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from os.path import dirname\n",
    "parent_dir = dirname(dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','2_hillslope_discharge'))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','3_channel_routing'))\n",
    "\n",
    "from vadoseZone import *\n",
    "import glob\n",
    "from groundwaterZone import *\n",
    "from REW import REW\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import mpld3\n",
    "import time\n",
    "import sys\n",
    "import shapely\n",
    "import fiona\n",
    "from pyDOE import *\n",
    "import folium\n",
    "from ast import literal_eval as make_tuple\n",
    "\n",
    "# Load config files, forcing file, and paramters for each group\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "sys.path.append(os.path.join(parent_dir, 'StreamflowTempModel', '1_data_preparation'))\n",
    "from prep import rew_params\n",
    "rew_params()\n",
    "\n",
    "# These dictionaries contain the all the data we'll need to instantiate \n",
    "rew_config = pickle.load( open( os.path.join(parent_dir,'model_data','rew_config.p'), \"rb\" ) )\n",
    "climate_group_forcing = pickle.load( open( os.path.join(parent_dir,'model_data','climate_group_forcing.p'), \"rb\" ) )\n",
    "parameter_group_params = pickle.load( open( os.path.join(parent_dir,'model_data','parameter_group_params.p'), \"rb\" ))\n",
    "model_config = pickle.load( open( os.path.join(parent_dir, 'model_data', 'model_config.p'), 'rb'))\n",
    "parameter_ranges = pickle.load( open( os.path.join(parent_dir, 'model_data', 'parameter_ranges.p'), 'rb'))\n",
    "\n",
    "start_date = model_config['start_date']\n",
    "stop_date = model_config['stop_date']\n",
    "spinup_date = model_config['spinup_date']\n",
    "Tmax = model_config['Tmax']\n",
    "dt = model_config['dt_hillslope']\n",
    "t = model_config['t_hillslope']\n",
    "resample_freq_hillslope = model_config['resample_freq_hillslope']\n",
    "timestamps_hillslope = model_config['timestamps_hillslope']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get REWs located within calibration sub-watershed \n",
    "\n",
    "Here, we use the representative REW points to determine which REWs are located within the sub-watershed that we are calibrating. We want to make sure to run the model only for the REWs that are relevant for calibration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWs [5] are located within the calibration sub-watershed\n",
      "The groups [(0, 4)] will be run for calibration purposes\n"
     ]
    }
   ],
   "source": [
    "subwatershed_name = 'elder'\n",
    "shapefile_path = os.path.join(parent_dir, 'raw_data','watershed_poly', subwatershed_name + '.shp')\n",
    "points = pd.read_csv(os.path.join(parent_dir, 'raw_data','basins_centroids', 'points.csv')).set_index('cat')\n",
    "\n",
    "# get coordinate tuples corresponding to each REW\n",
    "for index, row in points.iterrows():\n",
    "    new_tuple = make_tuple(points['coords'].loc[index])\n",
    "    points['coords'].loc[index] = new_tuple\n",
    "\n",
    "# check to see which REWs fall within sub-watershed\n",
    "ids_in_subwatershed = []\n",
    "with fiona.open(shapefile_path) as fiona_collection:\n",
    "    for shapefile_record in fiona_collection:\n",
    "        # Use Shapely to create the polygon\n",
    "        shape = shapely.geometry.Polygon( shapefile_record['geometry']['coordinates'][0] )\n",
    "\n",
    "        for index, row in points.iterrows(): \n",
    "            point = shapely.geometry.Point(row[0][0], row[0][1]) # longitude, latitude\n",
    "            # Alternative: if point.within(shape)\n",
    "            if shape.contains(point):\n",
    "                ids_in_subwatershed.append(index)\n",
    "\n",
    "ids_in_subwatershed = list(set(ids_in_subwatershed))\n",
    "\n",
    "# if no REWs found inside sub-watershed, assume the sub-watershed is contained within a single REW. \n",
    "# find the id of that REW. First get centroid of sub-watershed\n",
    "if len(ids_in_subwatershed)==0:\n",
    "    subwatershed_shape = gp.GeoDataFrame.from_file(shapefile_path)\n",
    "    subwatershed_point = subwatershed_shape['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "    with fiona.open(basins) as fiona_collection:\n",
    "        for shapefile_record in fiona_collection:\n",
    "            # Use Shapely to create the polygon\n",
    "            shape = shapely.geometry.Polygon( shapefile_record['geometry']['coordinates'][0] )\n",
    "            if shape.contains(point):\n",
    "                ids_in_subwatershed.append(shapefile_record['properties']['cat'])\n",
    "    \n",
    "groups_to_calibrate = []\n",
    "for rew_id in ids_in_subwatershed:\n",
    "    groups_to_calibrate.append(rew_config[rew_id]['group'])\n",
    "    \n",
    "    \n",
    "print('REWs %s are located within the calibration sub-watershed' % str(ids_in_subwatershed))\n",
    "print('The groups %s will be run for calibration purposes' % str(groups_to_calibrate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo procedure\n",
    "\n",
    "The hillslope calibrator will perform a simple Monte Carlo calibration on any parameters found in the `parameter_ranges` dictionary loaded above. For each iteration, parameters will be chosen using [latin hypercube sampling](https://en.wikipedia.org/wiki/Latin_hypercube_sampling) where the sampling endpoints of each parameter are determined from the values in `parameter_ranges`. For this particular calibration, the log Nash Sutcliffe will be computed: \n",
    "\n",
    "$$\\log NSE = 1 - \\frac{\\sum_t \\left|\\log O_t - \\log M_t \\right|}{ \\sum_t \\left| \\log O_t - \\log \\overline{O} \\right| } $$\n",
    "\n",
    "where $O_t$ denotes the observation at time $t$, $M_t$ denotes the corresponding modeled value, and $\\overline{O}$ denotes the overall observation mean.\n",
    "\n",
    "\n",
    "First, we specify the number (`N`) of calibration runs to perform. Each parameter realization is obtained using  to ensure adequate exploration of parameter space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective_function(modeled, observed):\n",
    "    inds = ((modeled != 0) & (observed != 0))\n",
    "    return 1 - np.sum(np.abs(np.log(observed.loc[inds]) - np.log(modeled.loc[inds])))/np.sum(np.abs(np.log(observed.loc[inds]) - np.log(np.mean(observed.loc[inds]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify the number of parameter sets to generate\n",
    "N = 10\n",
    "\n",
    "num_params = 0\n",
    "for parameter_group in parameter_ranges.values():\n",
    "    num_params += len(parameter_group)\n",
    "lhd = lhs(num_params,samples=N)\n",
    "\n",
    "# for each parameter realization\n",
    "solved_subwatersheds = []\n",
    "for i in range(len(lhd)):\n",
    "    solved_groups = {}\n",
    "    \n",
    "    # alter parameter set dictionary for each parameter set realization\n",
    "    # the lhs function produces parameter samples between 0 and 1, which \n",
    "    # must be re-scaled to the specified range for each parameter\n",
    "    for j, parameter_group in enumerate(parameter_ranges.keys()):\n",
    "        for k, parameter in enumerate(parameter_ranges[parameter_group].keys()):\n",
    "            new_value = lhd[i,j*k]*(parameter_ranges[parameter_group][parameter][1] - parameter_ranges[parameter_group][parameter][0]) + parameter_ranges[parameter_group][parameter][0]\n",
    "            parameter_group_params[parameter_group][parameter] = new_value\n",
    "\n",
    "            \n",
    "    solved_group_hillslopes_dict = {}\n",
    "    for group_id in groups_to_calibrate:\n",
    "\n",
    "        parameter_group_id = group_id[0]\n",
    "        climate_group_id = group_id[1]\n",
    "\n",
    "        vz = parameter_group_params[parameter_group_id]['vz'](**parameter_group_params[parameter_group_id])\n",
    "        gz = parameter_group_params[parameter_group_id]['gz'](**parameter_group_params[parameter_group_id])    \n",
    "\n",
    "        rew = REW(vz, gz,  **{'pet':climate_group_forcing[climate_group_id].pet, 'ppt':climate_group_forcing[climate_group_id].ppt, 'aspect':90})\n",
    "\n",
    "        storage    = np.zeros(np.size(t))\n",
    "        groundwater     = np.zeros(np.size(t))\n",
    "        discharge       = np.zeros(np.size(t))\n",
    "        leakage         = np.zeros(np.size(t))\n",
    "        ET              = np.zeros(np.size(t))\n",
    "\n",
    "        # Resample pet and ppt to integration timestep\n",
    "        ppt = np.array(rew.ppt[start_date:stop_date].resample(resample_freq_hillslope).ffill())\n",
    "        pet = np.array(rew.pet[start_date:stop_date].resample(resample_freq_hillslope).ffill())\n",
    "\n",
    "        # Solve group hillslope\n",
    "        for l in range(len(t)):\n",
    "            rew.vz.update(dt,**{'ppt':ppt[l],'pet':pet[l]})\n",
    "            storage[l] = rew.vz.storage\n",
    "            leakage[l]      = rew.vz.leakage\n",
    "            ET[l]           = rew.vz.ET   \n",
    "            rew.gz.update(dt,**{'leakage':leakage[l]})\n",
    "            groundwater[l] = rew.gz.storage\n",
    "            discharge[l] = rew.gz.discharge\n",
    "\n",
    "        # resample as daily data\n",
    "        solved_groups[group_id] = pd.DataFrame({'discharge':discharge}, index=timestamps_hillslope).resample('D').mean()\n",
    "        \n",
    "    total_area = 0\n",
    "    for rew_id in ids_in_subwatershed:\n",
    "        total_area += rew_config[rew_id]['area_sqkm']\n",
    "    \n",
    "    name = str(i) + 'discharge'\n",
    "    solved_subwatershed = pd.DataFrame({name:np.zeros(len(timestamps_hillslope))}, index=timestamps_hillslope).resample('D').mean()\n",
    " \n",
    "    solved_subwatershed_array = np.zeros(int(len(solved_subwatershed)))\n",
    "    for rew_id in ids_in_subwatershed:\n",
    "        solved_subwatershed_array += rew_config[rew_id]['area_sqkm']/total_area*solved_groups[rew_config[rew_id]['group']]['discharge']\n",
    "    \n",
    "    solved_subwatershed[name] = solved_subwatershed_array\n",
    "    solved_subwatersheds.append(solved_subwatershed)\n",
    "    \n",
    "solved_subwatersheds = pd.concat(solved_subwatersheds,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model goodness of fit\n",
    "\n",
    "Here, each model run is compared to calibration data using the objective function as defined above. The user must specify the pickled dataframe with calibration runoff data in units of cm/day. Calibration data must be available at least from `spinup_date` to `stop_date`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calibration_data_filename = 'elder_runoff.p'\n",
    "\n",
    "calibration_data = pickle.load( open(os.path.join(parent_dir,'calibration_data',calibration_data_filename)))\n",
    "calibration_data = calibration_data[spinup_date:stop_date]\n",
    "col_name = calibration_data.columns[0]\n",
    "calibration_data.columns = ['calibration_data']\n",
    "df = pd.concat([calibration_data, solved_subwatersheds],1)\n",
    "\n",
    "nses = []\n",
    "for i in range(len(lhd)):\n",
    "    name = str(i) + 'discharge'\n",
    "    if int(np.sum(df[name][spinup_date:stop_date])) == 0:\n",
    "        nses.append(-1)\n",
    "    else:\n",
    "        nses.append(objective_function( df['calibration_data'][spinup_date:stop_date], df[name][spinup_date:stop_date]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_column = str(np.argmax(nses)) + 'discharge'\n",
    "i = int(best_column.replace('discharge',''))\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.plot(df[['calibration_data',best_column]][spinup_date:stop_date])\n",
    "plt.legend(['Calibration data', 'Best model run (NSE = %0.2f)' % np.max(nses)])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Runoff [cm/day]')\n",
    "plt.title( subwatershed_name + ' subwatershed calibration results')\n",
    "html = mpld3.fig_to_html(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best fit parameter set has an NSE of 0.80\n",
      "The best fit value for parameter a in parameter group 0 is 0.000333\n",
      "The best fit value for parameter nR in parameter group 0 is 0.033277\n",
      "The best fit value for parameter zrS in parameter group 0 is 40.690558\n",
      "The best fit value for parameter zrR in parameter group 0 is 1146.579937\n"
     ]
    }
   ],
   "source": [
    "print 'The best fit parameter set has an NSE of %0.2f' % (np.max(nses))\n",
    "\n",
    "for j, parameter_group in enumerate(parameter_ranges.keys()):\n",
    "    for k, parameter in enumerate(parameter_ranges[parameter_group].keys()):\n",
    "        new_value = lhd[i,j*k]*(parameter_ranges[parameter_group][parameter][1] - parameter_ranges[parameter_group][parameter][0]) + parameter_ranges[parameter_group][parameter][0]\n",
    "        print 'The best fit value for parameter %s in parameter group %s is %f' % (parameter, parameter_group, new_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watershed_name = 'sf_miranda'\n",
    "subwatershed_name = 'elder'\n",
    "\n",
    "shapefile_path = os.path.join(parent_dir, 'raw_data','watershed_poly', subwatershed_name + '.shp')\n",
    "basins_shape = gp.GeoDataFrame.from_file(shapefile_path)\n",
    "basins_shape['coords'] = basins_shape['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "basins_shape['coords'] = [coords[0] for coords in basins_shape['coords']]\n",
    "basins = basins_shape.to_crs(epsg='4326').to_json()\n",
    "\n",
    "mapa = folium.Map([basins_shape['coords'][0][1], basins_shape['coords'][0][0]],\n",
    "                  zoom_start=11,\n",
    "                  tiles='Stamen Terrain')\n",
    "\n",
    "folium.GeoJson(\n",
    "    basins,\n",
    "    style_function=lambda feature: {\n",
    "        'color' : '#FF0000'\n",
    "        }\n",
    "    ).add_to(mapa)\n",
    "\n",
    "iframe = folium.element.IFrame(html=html, width=650, height=400)\n",
    "popup = folium.Popup(iframe, max_width=2650)\n",
    "folium.Marker([basins_shape['coords'][0][1], basins_shape['coords'][0][0]], popup=popup, icon=folium.Icon(color='red',icon='info-sign')).add_to(mapa)\n",
    "\n",
    "streams_path = glob.glob(os.path.join(parent_dir,'raw_data','streams_poly','*.shp'))[0]\n",
    "streams_shape = gp.GeoDataFrame.from_file(streams_path).to_crs(epsg='4326')\n",
    "streams = gp.GeoDataFrame(streams_shape['geometry'], crs=streams_shape.crs)\n",
    "streams['RGBA'] = '#0000ff'\n",
    "streams = streams.to_crs(epsg='4326').to_json()\n",
    "colors = []\n",
    "folium.GeoJson(\n",
    "    streams,\n",
    "    style_function=lambda feature: {\n",
    "        'color' : feature['properties']['RGBA'],\n",
    "        'weight' : 4, \n",
    "        'opacity': 1\n",
    "        }\n",
    "    ).add_to(mapa)\n",
    "\n",
    "#Add watershed\n",
    "shapefile_path = os.path.join(parent_dir, 'raw_data','watershed_poly', watershed_name + '.shp')\n",
    "basins_shape = gp.GeoDataFrame.from_file(shapefile_path)\n",
    "basins_shape['coords'] = basins_shape['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "basins_shape['coords'] = [coords[0] for coords in basins_shape['coords']]\n",
    "basins = basins_shape.to_crs(epsg='4326').to_json()\n",
    "\n",
    "folium.GeoJson(\n",
    "    basins,\n",
    "    style_function=lambda feature: {\n",
    "        'color' : '#00ff00',\n",
    "        'fillOpacity': .05\n",
    "        }\n",
    "    ).add_to(mapa)\n",
    "\n",
    "\n",
    "calibration_output_name = subwatershed_name + '_calibration.html'\n",
    "mapa.save(os.path.join(parent_dir, 'calibration_output', calibration_output_name))\n",
    "mapa.save(os.path.join(calibration_output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"600\" src=\"elder_calibration.html\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"600\" src=\"elder_calibration.html\"></iframe>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
