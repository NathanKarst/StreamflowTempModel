{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcing Extraction Tutorial\n",
    "\n",
    "In this notebook, we'll present an example of converting PRISM precipitation, mean temperature, and mean dew point data into a format that can be used easily in the types of vadose and groundwater zone models we cover in the [custom zone models tutorial](custom_zone_models.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gdal\n",
    "import os\n",
    "import numpy as np\n",
    "import fiona\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "from os.path import dirname\n",
    "import glob\n",
    "import sys\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "parent_dir = dirname(dirname(os.getcwd()))\n",
    "sys.path.append(os.path.join(parent_dir,'StreamflowTempModel','lib'))\n",
    "import zonal_stats as zs\n",
    "import meteolib as meteo\n",
    "import evaplib as evap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to specify the geographic areas of interest. We'll assume that these regions are specified via shapefiles located in the `raw_data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "except RuntimeError:\n",
    "    print 'Cannot find basins shapefile. Please make sure basins shapefile is located in \\n the model directory under /raw_data/basins_poly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we'll assume that the forcing along the spatial extent of each basin is constant and equal to the forcing observed at its centroid. We could also consider implementing zonal averaging or some other more faithful approximation, but this would be much more computationally intensive, and quite likely unnecessary for lower resolution climate datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc = fiona.open(basins)\n",
    "shapefile_record = fc.next()\n",
    "pos_dict={}\n",
    "for shapefile_record in fc:\n",
    "    shape = shapely.geometry.asShape(shapefile_record['geometry'])\n",
    "    long_point = shape.centroid.coords.xy[0][0]\n",
    "    lat_point = shape.centroid.coords.xy[1][0]\n",
    "    pos = (long_point, lat_point)\n",
    "    pos_dict[int(shapefile_record['properties']['cat'])]=pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the list of locations in hand, we can begin unpacking the forcing data. Here we'll focus on three time series: daily precipitation (`ppt`), mean daily temperature (`tmean`), and mean daily dew point (`tdmean`). \n",
    "\n",
    "Our main data structure here will be a dictionary whose keys are forcing data types and whose values are themselves dictionaries. These inner dictionaries have REW IDs as keys, and forcing data at particular instances of time as values.\n",
    "\n",
    "We track time in a separate dictionary, again keyed based on forcing data. The values here are lists of dates at which each type of forcing data was observed. \n",
    "\n",
    "Note: we assume that each forcing time series is stored exactly as would be downloaded from the PRISM download page. A simple bash script for downloading daily PRISM data can be found [HERE](https://github.com/daviddralle/downloadPrism). For the purposes of the model, the data should be stored in the `raw_data` folder in a foldername equal to the variable name, for instance precipitation (`ppt`) is in `raw_data/ppt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 ppt data processed.\n",
      "2013 ppt data processed.\n",
      "2014 ppt data processed.\n",
      "2012 tmean data processed.\n",
      "2013 tmean data processed.\n",
      "2014 tmean data processed.\n",
      "2012 tdmean data processed.\n",
      "2013 tdmean data processed.\n",
      "2014 tdmean data processed.\n"
     ]
    }
   ],
   "source": [
    "forcing_dict = {}\n",
    "dates_dict = {}\n",
    "\n",
    "prism_vars = ['ppt','tmean', 'tdmean']\n",
    "\n",
    "for prism_var in prism_vars:\n",
    "    years = os.listdir(os.path.join(parent_dir,'raw_data',prism_var))\n",
    "\n",
    "    # Initialize empty data structures -- a dict keyed on REW ID with empty lists as values, \n",
    "    # and an empty list that will hold dates at which prism_var was observed\n",
    "    vals_dict = {k:[] for k in pos_dict.keys()}\n",
    "    date_list=[]\n",
    "    \n",
    "    #for all years of forcing variable, load each day of raster data and extract to all REWs\n",
    "    for year in years:\n",
    "        try: \n",
    "            int(year)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        # Assuming that data is in .bil format from PRISM.\n",
    "        # Data for each REW is extracted to REW centroid lat/long. \n",
    "        raster_list = glob.glob(os.path.join(parent_dir,'raw_data',prism_var,year,'*.bil'))\n",
    "        for rast in raster_list:\n",
    "            date_list.append(rast[-16:][:8])\n",
    "            raster_file = os.path.join(parent_dir,'raw_data',prism_var,year,rast)\n",
    "            gdata = gdal.Open(raster_file)\n",
    "            gt = gdata.GetGeoTransform()\n",
    "            data = gdata.ReadAsArray().astype(np.float)\n",
    "            gdata = None\n",
    "            for rew_id in pos_dict.keys(): \n",
    "                pos = pos_dict[rew_id]\n",
    "                x = int((pos[0] - gt[0])/gt[1])\n",
    "                y = int((pos[1] - gt[3])/gt[5])\n",
    "                vals_dict[rew_id].append(data[y, x])\n",
    "                \n",
    "        print str(year) + ' ' + prism_var + ' data processed.'                \n",
    "\n",
    "    forcing_dict[prism_var] = vals_dict\n",
    "    dates_dict[prism_var] = date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the data for modeling, we need to ensure that the observation dates for each forcing time series align. We leverage Python's sets to make sure that all the datelists are in fact the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates match!\n"
     ]
    }
   ],
   "source": [
    "unique_dates = [list(i) for i in set(tuple(i) for i in dates_dict.values())]\n",
    "if len(unique_dates)>1:\n",
    "    print 'Forcing data dates do not all match, please check raw data!'\n",
    "else:\n",
    "    print 'Dates match!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we will house all timeseries data in Pandas dataframes in order to leverage some very nice resampling functionality. For now, we'll just initialize one empty dataframe for each REW. Each of these dataframes will be indexed by date and will have one column for each forcing timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = pd.date_range(unique_dates[0][0],unique_dates[0][-1])\n",
    "rew_dfs_dict = {k:pd.DataFrame(data=None,index=rng) for k in pos_dict.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first set the average daily net radiation, `rn`, in units of W/m$^2$. We'll assume that this has already been somehow computed (since net radiation is not a PRISM variable) and placed as a [pickled](https://docs.python.org/2/library/pickle.html) time series, `raw_data/rn.p`. Note that the net radiation time series must include at least the date range of the other forcing data, but could also include more. For this tutorial, the `rn` dataset is not spatially distributed and is considered the same for each REW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pickle.load( open( os.path.join(parent_dir, 'raw_data','rn', 'rn.p'), \"rb\" ) )\n",
    "rew_id = rew_dfs_dict.keys()[0]\n",
    "\n",
    "#get start stop dates of extracted PRISM data. Pull rn data from these dates. \n",
    "start = rew_dfs_dict[rew_id].index[0]\n",
    "stop = rew_dfs_dict[rew_id].index[-1]\n",
    "\n",
    "for rew_id in rew_dfs_dict.keys():\n",
    "    rew_dfs_dict[rew_id]['rn'] = df[start:stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simply cycle over the PRISM variables we enumerated earlier. Here, we might have to convert units to be in line with the main modeling effort's assumption that lengths are in units of cm and time is in units of days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for prism_var in prism_vars:\n",
    "    #need to convert ppt from mm/day to cm/day for PRISM data\n",
    "    if prism_var=='ppt': div = 10.0\n",
    "    else: div = 1.0\n",
    "    for rew_id in pos_dict.keys():\n",
    "        rew_dfs_dict[rew_id][prism_var] = np.array(forcing_dict[prism_var][rew_id])/div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last remaining forcing timeseries required by the model presented in this tutorial is daily average potential evapotranspiration (in units of cm/day). Here, we'll use the Priestly-Taylor equation to compute this forcing.   Before we get started, we need to build some helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute relative humidity using temp and dewpoint\n",
    "def get_rh(tmean, tdmean):\n",
    "    a = 17.625\n",
    "    b = 243.04\n",
    "    return 100*np.exp(a*tdmean/(b+tdmean))/np.exp(a*tmean/(b+tmean))\n",
    "\n",
    "#get average elevation of each REW. Uses zonal statistics library.\n",
    "def get_rew_elevations(basins, parent_dir):\n",
    "    #get mean rew elevation\n",
    "    dem_file = os.listdir(os.path.join(parent_dir,'raw_data','dem'))\n",
    "    dem_file = [x for x in dem_file if 'dem' in x]\n",
    "    elev_stats = zs.zonal_stats(basins, os.path.join(parent_dir,'raw_data','dem',dem_file[0]))\n",
    "    \n",
    "    rew_elevations = {}\n",
    "    translated = translate_to_rew_id(parent_dir)\n",
    "\n",
    "    for stat in elev_stats: \n",
    "        rew_elevations[translated[stat['fid']]] = stat['mean']\n",
    "\n",
    "    return rew_elevations\n",
    "\n",
    "def get_rew_pressures(rew_elevations):\n",
    "    #elevations in meters, output pressure in Pa\n",
    "    #using simple elevation/pressure relationship\n",
    "    rew_pressures = {}\n",
    "    for key in rew_elevations.keys():\n",
    "        rew_pressures[key] = 1000*101.325*((293-0.0065*rew_elevations[key])/293)**5.26\n",
    "\n",
    "    return rew_pressures\n",
    "\n",
    "# translate feature IDs to REW IDs\n",
    "def translate_to_fid(parent_dir):\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "    translated = {}\n",
    "\n",
    "    fc = fiona.open(basins)\n",
    "    shapefile_record = fc.next()\n",
    "    for shapefile_record in fc:\n",
    "        translated[int(shapefile_record['properties']['cat'])] = int(shapefile_record['id'])\n",
    "    return translated\n",
    "\n",
    "# translate REW IDs to feature IDs\n",
    "def translate_to_rew_id(parent_dir):\n",
    "    basins = glob.glob(os.path.join(parent_dir,'raw_data','basins_poly','*.shp'))[0]\n",
    "    translated = {}\n",
    "\n",
    "    fc = fiona.open(basins)\n",
    "    shapefile_record = fc.next()\n",
    "    for shapefile_record in fc:\n",
    "        translated[int(shapefile_record['id'])] = int(shapefile_record['properties']['cat'])\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have all the pieces to compute PET.. We'll use the Priestley-Taylor implementation from the `evap` module. We assume that the soil heat flux is zero at the daily timescale. Once all forcing timeseries have been computed, we save to `rew_forcing.p` in the `model_data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daviddralle/Dropbox/research/streamflow_temp/StreamflowTempModel/lib/zonal_stats.py:117: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  src_array == nodata_value,\n"
     ]
    }
   ],
   "source": [
    "rew_elevations = get_rew_elevations(basins, parent_dir)\n",
    "rew_pressures = get_rew_pressures(rew_elevations)\n",
    "for rew_id in pos_dict.keys():\n",
    "    rh = get_rh(rew_dfs_dict[rew_id]['tmean'], rew_dfs_dict[rew_id]['tdmean'])\n",
    "\n",
    "    pet = evap.Ept(\n",
    "        rew_dfs_dict[rew_id]['tmean'],\n",
    "        rh, \n",
    "        rew_pressures[rew_id]*np.ones(len(rh)), \n",
    "        rew_dfs_dict[rew_id]['rn'],\n",
    "        np.zeros(len(rh))\n",
    "        )\n",
    "    \n",
    "    rew_dfs_dict[rew_id]['pet'] = pet/10.0 # convert to cm/day\n",
    "\n",
    "pickle.dump( rew_dfs_dict, open( os.path.join(parent_dir,'model_data','rew_forcing.p'), \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
